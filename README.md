# Speech-Emotion-Detection
Detecting Emotion from Speech using MLP and CNN

This project proposes the recognition of speech emotions based on the acoustic features of the personâ€™s voice. The features include Mel Frequency Cepstral Coefficients (MFCC), Chroma Vector and Zero Crossing Rate. In this work, we test the benefit of sound augmentation techniques in speech emotion recognition. We have implemented algorithms like convolutional neural networks (CNN) and Multi-Layer Perceptron (MLP) are used to classify and predict the emotions based on the relevant features extracted from the speech signals. The model is tested on two datasets English datasets, RAVDESS and Surrey Audio-Visual Expressed Emotion (SAVEE). The CNN model has shown 86% accuracy and the MLP model has shown 79% accuracy.
