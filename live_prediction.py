# -*- coding: utf-8 -*-
"""live prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wRewZbRDRqviEvZ-CmI4SKZgefWfVbL8
"""

# IMPORT NECESSARY LIBRARIES
import librosa
import soundfile as sf
import librosa.display
from IPython.display import Audio
import numpy as np
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
import pandas as pd
import IPython.display as ipd  # To play sound in the notebook
import sys
import warnings
from tensorflow.keras.models import Sequential, save_model, load_model
from sklearn.preprocessing import LabelEncoder
import pickle
# ignore warnings 
if not sys.warnoptions:
    warnings.simplefilter("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)



mean = np.load('mean_std/mean.npy')
std = np.load('mean_std/std.npy')

#path = '/home/amrutha/Desktop/major project 2/speech_emotion_detection/CNN codes/dataset/ravdess/03-01-04-01-01-02-22.wav'
#path = '/home/amrutha/Downloads/record (3).wav'
#DATA AUGMENTATION
def feature_extraction(path):   
	# FUNCTION TO STRETCH THE SOUND
	def stretch(x, rate=0.8):
		data = librosa.effects.time_stretch(x, rate)
		return data

	# FUNCTION TO INCREASE SPEED AND PITCH 
	def speedNpitch(x):
		length_change = np.random.uniform(low=0.8, high = 1)
		speed_fac = 1.4 / length_change 
		tmp = np.interp(np.arange(0,len(x),speed_fac),np.arange(0,len(x)),x)
		minlen = min(x.shape[0], tmp.shape[0])
		x *= 0
		x[0:minlen] = tmp[0:minlen]
		return x

	# ADD STRETCH AND USE FEATURE EXTRACTION ON AUDIO FILES

	stretch_list = []
	X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=3,sr=44100,offset=0.5)
		# stretch
	X = stretch(X)
	aug = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)
	aug=np.mean(aug,axis=0)
	stretch_list = [aug]
	L_stretch=pd.DataFrame(stretch_list)

	audio_df4 = L_stretch
	audio_df4 = audio_df4.fillna(0)

	# ITERATE OVER ALL AUDIO FILES AND EXTRACT LOG MEL SPECTROGRAM MEAN VALUES INTO DF FOR MODELING 
	df = pd.DataFrame(columns=['mfcc','chroma','zcr'])
	X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration = 3, sr=44100,offset=0.5)

		
	# Mel-frequency cepstral coefficients (MFCCs)
	mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)
	mfcc=np.mean(mfcc,axis=0)

	# compute chroma energy (pertains to 12 different pitch classes)
	chroma = librosa.feature.chroma_stft(y=X, sr=sample_rate)
	chroma = np.mean(chroma, axis = 0)

	# compute zero-crossing-rate (zcr:the zcr is the rate of sign changes along a signal i.e.m the rate at 
	#     which the signal changes from positive to negative or back - separation of voiced andunvoiced speech.)
	zcr = librosa.feature.zero_crossing_rate(y=X)
	zcr = np.mean(zcr, axis= 0)

	df.loc[0] = [mfcc,chroma, zcr]

	zcr= pd.DataFrame(df['zcr'].values.tolist())
	chroma= pd.DataFrame(df['chroma'].values.tolist())
	mfcc= pd.DataFrame(df['mfcc'].values.tolist())

	df_combined_2 = pd.concat([audio_df4,chroma,zcr],axis=1)
	


	X_data = df_combined_2
	X_data = X_data.fillna(0)

	# NORMALIZE DATA
	X_data = (X_data - mean)/std
	
	return X_data
	
def predict_emotion_mlp(X_data):

	"""##MLP Prediction"""

	mlp_model = pickle.load(open("mlp_model/mlp_classifier.model", "rb"))
	result = mlp_model.predict(X_data)
	print("MLP Result: ",result[0])
	return result[0]

def predict_emotion_cnn(X_data):
	"""##CNN Prediction"""
	cnn_model = load_model('cnn_model/', compile = True)
	X_data_cnn = np.array(X_data)
	X_data_cnn = X_data_cnn[:,:,np.newaxis]

	x = np.argmax(cnn_model.predict(X_data_cnn), axis=-1)
	lb = LabelEncoder()
	emotions = ['angry', 'calm', 'disgust' ,'fear' ,'happy', 'neutral' ,'sad', 'surprise']
	lb.fit(emotions)

	cnn_result = (lb.inverse_transform((x)))
	print("CNN Result: ", cnn_result[0])

#X_data = feature_extraction(path)
#predict_emotion_mlp(X_data)
#predict_emotion_cnn(X_data)
